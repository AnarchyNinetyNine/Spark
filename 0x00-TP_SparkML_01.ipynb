{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary PySpark modules and classes for data handling and ML\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/19 01:51:56 WARN Utils: Your hostname, Precision-M6700 resolves to a loopback address: 127.0.1.1; using 192.168.43.246 instead (on interface wlp3s0)\n",
      "25/05/19 01:51:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/19 01:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create or retrieve a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Product Data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for a sample product dataset\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"prix_eur\", FloatType(), False),\n",
    "    StructField(\"technologie\", StringType(), False),\n",
    "    StructField(\"score\", FloatType(), False),\n",
    "    StructField(\"statut\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Sample data for the products\n",
    "data = [\n",
    "    (101, 1499.99, \"Laptop\", 8.7, \"en vente\"),\n",
    "    (102, 199.00, \"Drone\", 9.1, \"epuise\"),\n",
    "    (103, 749.50, \"Laptop\", 7.9, \"en vente\"),\n",
    "    (104, 299.95, \"Drone\", 8.3, \"epuise\"),\n",
    "    (105, 129.99, \"Smartwatch\", 7.5, \"en vente\"),\n",
    "    (106, 999.00, \"Laptop\", 9.4, \"epuise\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using the schema and sample data\n",
    "df = spark.createDataFrame(data, schema=schema, verifySchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- prix_eur: float (nullable = false)\n",
      " |-- technologie: string (nullable = false)\n",
      " |-- score: float (nullable = false)\n",
      " |-- statut: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+-----+--------+\n",
      "|id |prix_eur|technologie|score|statut  |\n",
      "+---+--------+-----------+-----+--------+\n",
      "|101|1499.99 |Laptop     |8.7  |en vente|\n",
      "|102|199.0   |Drone      |9.1  |epuise  |\n",
      "|103|749.5   |Laptop     |7.9  |en vente|\n",
      "|104|299.95  |Drone      |8.3  |epuise  |\n",
      "|105|129.99  |Smartwatch |7.5  |en vente|\n",
      "|106|999.0   |Laptop     |9.4  |epuise  |\n",
      "+---+--------+-----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------------+-----+----------+\n",
      "|id |prix_eur|technologie_idx|score|statut_idx|\n",
      "+---+--------+---------------+-----+----------+\n",
      "|101|1499.99 |2.0            |8.7  |0.0       |\n",
      "|102|199.0   |1.0            |9.1  |1.0       |\n",
      "|103|749.5   |2.0            |7.9  |0.0       |\n",
      "|104|299.95  |1.0            |8.3  |1.0       |\n",
      "|105|129.99  |0.0            |7.5  |0.0       |\n",
      "|106|999.0   |2.0            |9.4  |1.0       |\n",
      "+---+--------+---------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index categorical columns: 'technologie' and 'statut'\n",
    "tech_indexer = StringIndexer(inputCol=\"technologie\", outputCol=\"technologie_idx\", stringOrderType=\"frequencyAsc\")\n",
    "statut_indexer = StringIndexer(inputCol=\"statut\", outputCol=\"statut_idx\", stringOrderType=\"frequencyAsc\")\n",
    "\n",
    "# Create a pipeline for indexers\n",
    "pipeline = Pipeline(stages=[tech_indexer, statut_indexer])\n",
    "indexed_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Show indexed columns\n",
    "indexed_df.select(\"id\", \"prix_eur\", \"technologie_idx\", \"score\", \"statut_idx\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------------+---------------+-----+----------+-------------+\n",
      "|id |prix_eur|technologie_idx|technologie_vec|score|statut_idx|statut_vec   |\n",
      "+---+--------+---------------+---------------+-----+----------+-------------+\n",
      "|101|1499.99 |2.0            |(2,[],[])      |8.7  |0.0       |(1,[0],[1.0])|\n",
      "|102|199.0   |1.0            |(2,[1],[1.0])  |9.1  |1.0       |(1,[],[])    |\n",
      "|103|749.5   |2.0            |(2,[],[])      |7.9  |0.0       |(1,[0],[1.0])|\n",
      "|104|299.95  |1.0            |(2,[1],[1.0])  |8.3  |1.0       |(1,[],[])    |\n",
      "|105|129.99  |0.0            |(2,[0],[1.0])  |7.5  |0.0       |(1,[0],[1.0])|\n",
      "|106|999.0   |2.0            |(2,[],[])      |9.4  |1.0       |(1,[],[])    |\n",
      "+---+--------+---------------+---------------+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode indexed categorical columns\n",
    "ohe_encoder = OneHotEncoder(\n",
    "    inputCols=[\"technologie_idx\", \"statut_idx\"],\n",
    "    outputCols=[\"technologie_vec\", \"statut_vec\"]\n",
    ")\n",
    "\n",
    "# Extend pipeline to include encoding\n",
    "pipeline = Pipeline(stages=[tech_indexer, statut_indexer, ohe_encoder])\n",
    "encoded_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Show one-hot encoded columns\n",
    "encoded_df.select(\n",
    "    \"id\", \"prix_eur\", \"technologie_idx\", \"technologie_vec\",\n",
    "    \"score\", \"statut_idx\", \"statut_vec\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|101|[1499.98999023437...|\n",
      "|102|[199.0,1.0,9.1000...|\n",
      "|103|[749.5,2.0,7.9000...|\n",
      "|104|[299.950012207031...|\n",
      "|105|[129.990005493164...|\n",
      "|106|[999.0,2.0,9.3999...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assemble selected columns into a feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"prix_eur\", \"technologie_idx\", \"score\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Apply transformation\n",
    "output = assembler.transform(indexed_df)\n",
    "\n",
    "# Show resulting features\n",
    "output.select(col(\"id\"), col(\"features\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "csv_path = \"file://\" + os.path.abspath(\"heart.csv\")\n",
    "data = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(csv_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+\n",
      "|age|sex|cp |trtbps|chol|fbs|restecg|thalachh|exng|oldpeak|slp|caa|thall|output|\n",
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+\n",
      "|63 |1  |3  |145   |233 |1  |0      |150     |0   |2.3    |0  |0  |1    |1     |\n",
      "|37 |1  |2  |130   |250 |0  |1      |187     |0   |3.5    |0  |0  |2    |1     |\n",
      "|41 |0  |1  |130   |204 |0  |0      |172     |0   |1.4    |2  |0  |2    |1     |\n",
      "|56 |1  |1  |120   |236 |0  |1      |178     |0   |0.8    |2  |0  |2    |1     |\n",
      "|57 |0  |0  |120   |354 |0  |1      |163     |1   |0.6    |2  |0  |2    |1     |\n",
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample rows\n",
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trtbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalachh: integer (nullable = true)\n",
      " |-- exng: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slp: integer (nullable = true)\n",
      " |-- caa: integer (nullable = true)\n",
      " |-- thall: integer (nullable = true)\n",
      " |-- output: integer (nullable = true)\n",
      "\n",
      "+--------+-------------+\n",
      "|column  |unique_values|\n",
      "+--------+-------------+\n",
      "|age     |41           |\n",
      "|sex     |2            |\n",
      "|cp      |4            |\n",
      "|trtbps  |49           |\n",
      "|chol    |152          |\n",
      "|fbs     |2            |\n",
      "|restecg |3            |\n",
      "|thalachh|91           |\n",
      "|exng    |2            |\n",
      "|oldpeak |40           |\n",
      "|slp     |3            |\n",
      "|caa     |5            |\n",
      "|thall   |4            |\n",
      "|output  |2            |\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary SQL view\n",
    "data.createOrReplaceTempView(\"data\")\n",
    "\n",
    "# Count rows and columns\n",
    "nb_lines = data.count()\n",
    "nb_columns = len(data.columns)\n",
    "\n",
    "# Print schema\n",
    "data.printSchema()\n",
    "\n",
    "# Count unique values per column\n",
    "unique_counts = [(col, data.select(col).distinct().count()) for col in data.columns]\n",
    "unique_counts_df = spark.createDataFrame([Row(column=col, unique_values=count) for col, count in unique_counts])\n",
    "unique_counts_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/19 02:03:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|               age|            trtbps|              chol|          thalachh|           oldpeak|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|               303|               303|               303|               303|               303|\n",
      "|   mean|54.366336633663366|131.62376237623764|246.26402640264027|149.64686468646866|1.0396039603960396|\n",
      "| stddev|  9.08210098983786|  17.5381428135171| 51.83075098793005| 22.90516111491409|1.1610750220686346|\n",
      "|    min|                29|                94|               126|                71|               0.0|\n",
      "|    max|                77|               200|               564|               202|               6.2|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate columns into categorical and constant (numerical)\n",
    "categorical_cols = [\"sex\", \"exng\", \"caa\", \"cp\", \"fbs\", \"restecg\", \"slp\", \"thall\"]\n",
    "constant_cols = [\"age\", \"trtbps\", \"chol\", \"thalachh\", \"oldpeak\"]\n",
    "target_col = [\"output\"]\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "data.select(constant_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+\n",
      "|age|sex| cp|trtbps|chol|fbs|restecg|thalachh|exng|oldpeak|slp|caa|thall|output|\n",
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+\n",
      "|  0|  0|  0|     0|   0|  0|      0|       0|   0|      0|  0|  0|    0|     0|\n",
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+\n",
      "\n",
      "+------+-----+\n",
      "|output|count|\n",
      "+------+-----+\n",
      "|     1|  165|\n",
      "|     0|  138|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count nulls in each column\n",
    "null_counts_df = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "null_counts_df.show()\n",
    "\n",
    "# Count total and distinct rows\n",
    "total_rows = data.count()\n",
    "distinct_rows = data.distinct().count()\n",
    "duplicated_rows = total_rows - distinct_rows\n",
    "\n",
    "# Show distribution of target variable\n",
    "data.groupBy(\"output\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/19 02:03:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.0984466 , -0.06865302,  0.27935091,  0.21367796,\n",
       "         0.12130765, -0.1162109 , -0.39852194,  0.09680083,  0.21001257,\n",
       "        -0.16881424,  0.27632624,  0.06800138, -0.22543872],\n",
       "       [-0.0984466 ,  1.        , -0.04935288, -0.05676882, -0.19791217,\n",
       "         0.04503179, -0.05819627, -0.04401991,  0.14166381,  0.09609288,\n",
       "        -0.03071057,  0.11826141,  0.2100411 , -0.28093658],\n",
       "       [-0.06865302, -0.04935288,  1.        ,  0.04760776, -0.07690439,\n",
       "         0.09444403,  0.04442059,  0.29576212, -0.39428027, -0.14923016,\n",
       "         0.11971659, -0.18105303, -0.16173557,  0.43379826],\n",
       "       [ 0.27935091, -0.05676882,  0.04760776,  1.        ,  0.12317421,\n",
       "         0.17753054, -0.11410279, -0.04669773,  0.06761612,  0.19321647,\n",
       "        -0.12147458,  0.10138899,  0.06220989, -0.14493113],\n",
       "       [ 0.21367796, -0.19791217, -0.07690439,  0.12317421,  1.        ,\n",
       "         0.0132936 , -0.15104008, -0.00993984,  0.06702278,  0.05395192,\n",
       "        -0.00403777,  0.07051093,  0.09880299, -0.08523911],\n",
       "       [ 0.12130765,  0.04503179,  0.09444403,  0.17753054,  0.0132936 ,\n",
       "         1.        , -0.08418905, -0.00856711,  0.02566515,  0.00574722,\n",
       "        -0.05989418,  0.13797933, -0.03201934, -0.02804576],\n",
       "       [-0.1162109 , -0.05819627,  0.04442059, -0.11410279, -0.15104008,\n",
       "        -0.08418905,  1.        ,  0.04412344, -0.07073286, -0.05877023,\n",
       "         0.09304482, -0.07204243, -0.0119814 ,  0.1372295 ],\n",
       "       [-0.39852194, -0.04401991,  0.29576212, -0.04669773, -0.00993984,\n",
       "        -0.00856711,  0.04412344,  1.        , -0.37881209, -0.34418695,\n",
       "         0.38678441, -0.21317693, -0.09643913,  0.42174093],\n",
       "       [ 0.09680083,  0.14166381, -0.39428027,  0.06761612,  0.06702278,\n",
       "         0.02566515, -0.07073286, -0.37881209,  1.        ,  0.28822281,\n",
       "        -0.25774837,  0.11573938,  0.20675379, -0.43675708],\n",
       "       [ 0.21001257,  0.09609288, -0.14923016,  0.19321647,  0.05395192,\n",
       "         0.00574722, -0.05877023, -0.34418695,  0.28822281,  1.        ,\n",
       "        -0.57753682,  0.22268232,  0.21024413, -0.430696  ],\n",
       "       [-0.16881424, -0.03071057,  0.11971659, -0.12147458, -0.00403777,\n",
       "        -0.05989418,  0.09304482,  0.38678441, -0.25774837, -0.57753682,\n",
       "         1.        , -0.08015521, -0.10476379,  0.34587708],\n",
       "       [ 0.27632624,  0.11826141, -0.18105303,  0.10138899,  0.07051093,\n",
       "         0.13797933, -0.07204243, -0.21317693,  0.11573938,  0.22268232,\n",
       "        -0.08015521,  1.        ,  0.15183213, -0.39172399],\n",
       "       [ 0.06800138,  0.2100411 , -0.16173557,  0.06220989,  0.09880299,\n",
       "        -0.03201934, -0.0119814 , -0.09643913,  0.20675379,  0.21024413,\n",
       "        -0.10476379,  0.15183213,  1.        , -0.34402927],\n",
       "       [-0.22543872, -0.28093658,  0.43379826, -0.14493113, -0.08523911,\n",
       "        -0.02804576,  0.1372295 ,  0.42174093, -0.43675708, -0.430696  ,\n",
       "         0.34587708, -0.39172399, -0.34402927,  1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assemble all features into one vector for correlation analysis\n",
    "assembler = VectorAssembler(inputCols=data.columns, outputCol=\"features\")\n",
    "vector_df = assembler.transform(data).select(\"features\")\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = Correlation.corr(vector_df, \"features\").head()[0]\n",
    "corr_array = correlation_matrix.toArray()\n",
    "corr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicate rows if any\n",
    "data.count()\n",
    "data = data.dropDuplicates()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+--------------------+--------------------+\n",
      "|age|sex| cp|trtbps|chol|fbs|restecg|thalachh|exng|oldpeak|slp|caa|thall|output|            features|     scaled_features|\n",
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+--------------------+--------------------+\n",
      "| 69|  0|  3|   140| 239|  0|      1|     151|   0|    1.8|  2|  2|    2|     1|[0.0,0.0,2.0,3.0,...|[-1.4624390328984...|\n",
      "| 53|  0|  0|   130| 264|  0|      0|     143|   0|    0.4|  1|  0|    2|     1|(13,[6,7,8,9,10,1...|[-1.4624390328984...|\n",
      "| 54|  1|  2|   125| 273|  0|      0|     152|   0|    0.5|  0|  1|    2|     1|[1.0,0.0,1.0,2.0,...|[0.68152498620508...|\n",
      "| 51|  1|  0|   140| 298|  0|      1|     122|   1|    4.2|  1|  3|    3|     0|[1.0,1.0,3.0,0.0,...|[0.68152498620508...|\n",
      "| 58|  0|  1|   136| 319|  1|      0|     152|   0|    0.0|  2|  2|    2|     0|[0.0,0.0,2.0,1.0,...|[-1.4624390328984...|\n",
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assemble selected features and scale them\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=categorical_cols + constant_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Pipeline for assembling and scaling\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "scaled_data = pipeline.fit(data).transform(data)\n",
    "\n",
    "# Show scaled features\n",
    "scaled_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "train_data, test_data = scaled_data.randomSplit([0.8, 0.2], seed=1234)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
